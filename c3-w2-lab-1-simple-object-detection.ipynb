{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple Object Detection in Tensorflow\n\nThis lab will walk you through how to use object detection models available in [Tensorflow Hub](https://www.tensorflow.org/hub). In the following sections, you will:\n\n* explore the Tensorflow Hub for object detection models\n* load the models in your workspace\n* preprocess an image for inference \n* run inference on the models and inspect the output\n\nLet's get started!","metadata":{"id":"mmANPR2jhCR6"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"8DkMLuGDhCR6"}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom PIL import Image\nfrom PIL import ImageOps\nimport tempfile\nfrom six.moves.urllib.request import urlopen\nfrom six import BytesIO","metadata":{"id":"OEoRKdmByrb0","execution":{"iopub.status.busy":"2023-06-04T02:11:24.891145Z","iopub.execute_input":"2023-06-04T02:11:24.891527Z","iopub.status.idle":"2023-06-04T02:11:32.722667Z","shell.execute_reply.started":"2023-06-04T02:11:24.891489Z","shell.execute_reply":"2023-06-04T02:11:32.721706Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Download the model from Tensorflow Hub\n\nTensorflow Hub is a repository of trained machine learning models which you can reuse in your own projects. \n- You can see the domains covered [here](https://tfhub.dev/) and its subcategories. \n- For this lab, you will want to look at the [image object detection subcategory](https://tfhub.dev/s?module-type=image-object-detection). \n- You can select a model to see more information about it and copy the URL so you can download it to your workspace. \n- We selected a [inception resnet version 2](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)\n- You can also modify this following cell to choose the other model that we selected, [ssd mobilenet version 2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2)","metadata":{"id":"nb8MBgTOhCR6"}},{"cell_type":"code","source":"# you can switch the commented lines here to pick the other model\n\n# inception resnet version 2\nmodule_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n\n# You can choose ssd mobilenet version 2 instead and compare the results\n#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\"","metadata":{"id":"C9pCzz4uy20U","execution":{"iopub.status.busy":"2023-06-04T02:11:44.559223Z","iopub.execute_input":"2023-06-04T02:11:44.560088Z","iopub.status.idle":"2023-06-04T02:11:44.564997Z","shell.execute_reply.started":"2023-06-04T02:11:44.560045Z","shell.execute_reply":"2023-06-04T02:11:44.564200Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Load the model\n\nNext, you'll load the model specified by the `module_handle`.\n- This will take a few minutes to load the model.","metadata":{"id":"W3trj5FbhCR6"}},{"cell_type":"code","source":"model = hub.load(module_handle)","metadata":{"id":"0WHkGDHfhCR6","execution":{"iopub.status.busy":"2023-06-04T02:11:50.398227Z","iopub.execute_input":"2023-06-04T02:11:50.398900Z","iopub.status.idle":"2023-06-04T02:13:18.510444Z","shell.execute_reply.started":"2023-06-04T02:11:50.398865Z","shell.execute_reply":"2023-06-04T02:13:18.509476Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Choose the default signature\n\nSome models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what *signature* to use when running the model. \n- If you want to see if a model has more than one signature then you can do something like `print(hub.load(module_handle).signatures.keys())`. In your case, the models you will be using only have the `default` signature so you don't have to worry about other types.","metadata":{"id":"1Ey0FpHGhCR6"}},{"cell_type":"code","source":"# take a look at the available signatures for this particular model\nmodel.signatures.keys()","metadata":{"id":"X1BU7AGthCR6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"54ee436b-83e7-4f93-aa52-0db28047f003","execution":{"iopub.status.busy":"2023-06-04T02:13:18.512405Z","iopub.execute_input":"2023-06-04T02:13:18.512842Z","iopub.status.idle":"2023-06-04T02:13:18.521041Z","shell.execute_reply.started":"2023-06-04T02:13:18.512809Z","shell.execute_reply":"2023-06-04T02:13:18.519999Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"KeysView(_SignatureMap({'default': <ConcreteFunction pruned(images) at 0x7F5D9BE30790>}))"},"metadata":{}}]},{"cell_type":"markdown","source":"Please choose the 'default' signature for your object detector.\n- For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what you'll want here.","metadata":{"id":"nfc9ax9hhCR6"}},{"cell_type":"code","source":"detector = model.signatures['default']","metadata":{"id":"pzwR5zE_hCR7","execution":{"iopub.status.busy":"2023-06-04T02:14:37.369908Z","iopub.execute_input":"2023-06-04T02:14:37.370449Z","iopub.status.idle":"2023-06-04T02:14:37.374931Z","shell.execute_reply.started":"2023-06-04T02:14:37.370407Z","shell.execute_reply":"2023-06-04T02:14:37.374058Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### download_and_resize_image\n\nThis function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk.","metadata":{"id":"Wvb-3r3thCR7"}},{"cell_type":"code","source":"def download_and_resize_image(url, new_width=256, new_height=256):\n    '''\n    Fetches an image online, resizes it and saves it locally.\n    \n    Args:\n        url (string) -- link to the image\n        new_width (int) -- size in pixels used for resizing the width of the image\n        new_height (int) -- size in pixels used for resizing the length of the image\n        \n    Returns:\n        (string) -- path to the saved image\n    '''\n    \n    \n    # create a temporary file ending with \".jpg\"\n    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n    \n    # opens the given URL\n    response = urlopen(url)\n    \n    # reads the image fetched from the URL\n    image_data = response.read()\n    \n    # puts the image data in memory buffer\n    image_data = BytesIO(image_data)\n    \n    # opens the image\n    pil_image = Image.open(image_data)\n    \n    # resizes the image. will crop if aspect ratio is different.\n    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n    \n    # converts to the RGB colorspace\n    pil_image_rgb = pil_image.convert(\"RGB\")\n    \n    # saves the image to the temporary file created earlier\n    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n    \n    print(\"Image downloaded to %s.\" % filename)\n    \n    return filename","metadata":{"id":"Ucsxak_qhCR7","execution":{"iopub.status.busy":"2023-06-04T02:14:40.402220Z","iopub.execute_input":"2023-06-04T02:14:40.402899Z","iopub.status.idle":"2023-06-04T02:14:40.410532Z","shell.execute_reply.started":"2023-06-04T02:14:40.402865Z","shell.execute_reply":"2023-06-04T02:14:40.409607Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Download and preprocess an image\n\nNow, using `download_and_resize_image` you can get a sample image online and save it locally. \n- We've provided a URL for you, but feel free to choose another image to run through the object detector.\n- You can use the original width and height of the image but feel free to modify it and see what results you get.","metadata":{"id":"r7qodEJHhCR7"}},{"cell_type":"code","source":"# You can choose a different URL that points to an image of your choice\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n\n# download the image and use the original height and width\ndownloaded_image_path = download_and_resize_image(image_url, 3872, 2592)","metadata":{"id":"xHTDalVrhCR7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ee2933d-2eeb-4ed4-9995-8524456c9322","execution":{"iopub.status.busy":"2023-06-04T02:14:45.361061Z","iopub.execute_input":"2023-06-04T02:14:45.361434Z","iopub.status.idle":"2023-06-04T02:14:45.732398Z","shell.execute_reply.started":"2023-06-04T02:14:45.361404Z","shell.execute_reply":"2023-06-04T02:14:45.730648Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Image downloaded to /tmp/tmp_g8y0wle.jpg.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/3150208785.py:31: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### run_detector\n\nThis function will take in the object detection model `detector` and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n- run_detector uses `load_image` to convert the image into a tensor.","metadata":{"id":"IVNXUKMIhCR7"}},{"cell_type":"code","source":"def load_img(path):\n    '''\n    Loads a JPEG image and converts it to a tensor.\n    \n    Args:\n        path (string) -- path to a locally saved JPEG image\n    \n    Returns:\n        (tensor) -- an image tensor\n    '''\n    \n    # read the file\n    img = tf.io.read_file(path)\n    \n    # convert to a tensor\n    img = tf.image.decode_jpeg(img, channels=3)\n    \n    return img\n\n\ndef run_detector(detector, path):\n    '''\n    Runs inference on a local file using an object detection model.\n    \n    Args:\n        detector (model) -- an object detection model loaded from TF Hub\n        path (string) -- path to an image saved locally\n    '''\n    \n    # load an image tensor from a local file path\n    img = load_img(path)\n\n    # add a batch dimension in front of the tensor\n    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n    \n    # run inference using the model\n    result = detector(converted_img)\n\n    # save the results in a dictionary\n    result = {key:value.numpy() for key,value in result.items()}\n\n    # print results\n    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n\n    print(result[\"detection_scores\"])\n    print(result[\"detection_class_entities\"])\n    print(result[\"detection_boxes\"])\n","metadata":{"id":"wkkiQzKlhCR7","execution":{"iopub.status.busy":"2023-06-04T02:14:50.040206Z","iopub.execute_input":"2023-06-04T02:14:50.040895Z","iopub.status.idle":"2023-06-04T02:14:50.049373Z","shell.execute_reply.started":"2023-06-04T02:14:50.040858Z","shell.execute_reply":"2023-06-04T02:14:50.048392Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Run inference on the image\n\nYou can run your detector by calling the `run_detector` function. This will print the number of objects found followed by three lists: \n\n* The detection scores of each object found (i.e. how confident the model is), \n* The classes of each object found, \n* The bounding boxes of each object\n\nYou will see how to overlay this information on the original image in the next sections and in this week's assignment!","metadata":{"id":"DSEeJSkxhCR7"}},{"cell_type":"code","source":"# runs the object detection model and prints information about the objects found\nrun_detector(detector, downloaded_image_path)","metadata":{"id":"csanHvDIz4_t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b42e0e6-e87c-4e3f-9229-bc2c7a37a914","execution":{"iopub.status.busy":"2023-06-04T02:14:59.187391Z","iopub.execute_input":"2023-06-04T02:14:59.187759Z","iopub.status.idle":"2023-06-04T02:15:50.043974Z","shell.execute_reply.started":"2023-06-04T02:14:59.187731Z","shell.execute_reply":"2023-06-04T02:15:50.042353Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Found 100 objects.\n[0.6544859  0.6114536  0.60422754 0.5926311  0.5921924  0.5804915\n 0.5514062  0.49466896 0.47515774 0.47342235 0.43995944 0.41485173\n 0.40629673 0.39828938 0.3976528  0.37621054 0.37279457 0.365748\n 0.35260627 0.33274752 0.30428624 0.27276656 0.2686506  0.2577712\n 0.2529058  0.24612091 0.23403816 0.20342892 0.1822944  0.18045755\n 0.17571315 0.1643509  0.15850012 0.15666007 0.1547094  0.1545275\n 0.14924884 0.13340692 0.12948276 0.12649721 0.12044189 0.1176737\n 0.11356063 0.11114834 0.11100277 0.10914955 0.10604029 0.08940485\n 0.08598281 0.08280165 0.0810452  0.0780609  0.07760125 0.07628632\n 0.07546879 0.07444117 0.07427187 0.07204881 0.07177544 0.07102233\n 0.07032738 0.06809716 0.06304486 0.06285892 0.06270956 0.06223943\n 0.0588212  0.05814994 0.05795758 0.0578757  0.05462386 0.05274335\n 0.05133715 0.04826549 0.04708407 0.0468297  0.04495224 0.04405158\n 0.04360722 0.04113497 0.04109965 0.03968568 0.03934931 0.03912789\n 0.03879525 0.03878606 0.03739642 0.03606929 0.03367097 0.03366856\n 0.03260199 0.03253521 0.03201484 0.02983089 0.02877995 0.02867628\n 0.02803945 0.02783184 0.02734381 0.02668228]\n[b'Person' b'Person' b'Person' b'Person' b'Footwear' b'Person' b'Building'\n b'Bicycle' b'Building' b'Window' b'Person' b'Bicycle' b'Wheel'\n b'Building' b'Building' b'Building' b'Person' b'Wheel' b'Window'\n b'Window' b'Building' b'Person' b'Van' b'Person' b'Bicycle wheel'\n b'Person' b'Window' b'Window' b'Building' b'Window' b'Window' b'Man'\n b'Person' b'Woman' b'Person' b'Clothing' b'Bicycle wheel' b'Window'\n b'Person' b'Window' b'Land vehicle' b'Land vehicle' b'Clothing' b'Window'\n b'Bicycle' b'Land vehicle' b'House' b'House' b'Man' b'Window' b'Clothing'\n b'Window' b'Footwear' b'Person' b'Man' b'Man' b'House' b'Building'\n b'Person' b'Clothing' b'Window' b'Person' b'Man' b'Person' b'Furniture'\n b'Jeans' b'Person' b'Person' b'Person' b'Land vehicle' b'Window' b'House'\n b'Woman' b'Man' b'Window' b'Person' b'Person' b'Clothing' b'Man' b'Man'\n b'Window' b'Car' b'Person' b'Man' b'Chair' b'Car' b'House' b'Window'\n b'Tire' b'Clothing' b'Window' b'Clothing' b'Land vehicle' b'Window'\n b'Window' b'Man' b'Van' b'Bus' b'Clothing' b'Car']\n[[5.12794435e-01 5.29270947e-01 6.01662338e-01 5.52094460e-01]\n [5.19746006e-01 6.01507127e-01 6.46124184e-01 6.34682894e-01]\n [5.05746007e-01 5.00440776e-01 6.01349175e-01 5.23089647e-01]\n [4.86308753e-01 4.12762254e-01 6.78550363e-01 4.59905654e-01]\n [8.15190852e-01 9.56118405e-01 8.42701733e-01 9.87144709e-01]\n [4.95466411e-01 9.23534274e-01 8.35634887e-01 9.99056876e-01]\n [1.10986074e-02 1.19120395e-02 7.39750385e-01 4.24907178e-01]\n [5.77825963e-01 3.66453260e-01 7.12805748e-01 4.83338177e-01]\n [7.74935707e-02 4.13054079e-01 5.79458833e-01 5.60309231e-01]\n [0.00000000e+00 1.19292587e-01 2.23897204e-01 1.83949053e-01]\n [5.14069736e-01 7.48097837e-01 5.91962218e-01 7.66569197e-01]\n [5.70777833e-01 3.61820370e-01 7.07328439e-01 4.29666817e-01]\n [6.32094085e-01 3.59869897e-01 7.03841746e-01 4.11815584e-01]\n [1.59085598e-02 6.84961617e-01 5.59388876e-01 8.11146796e-01]\n [0.00000000e+00 7.97109365e-01 6.73736095e-01 1.00000000e+00]\n [0.00000000e+00 2.17026979e-01 6.50973141e-01 4.32000935e-01]\n [5.00372767e-01 3.77004474e-01 6.33350492e-01 4.14514393e-01]\n [6.40339971e-01 4.45023388e-01 7.03034759e-01 4.83457476e-01]\n [1.94403331e-03 0.00000000e+00 1.39331967e-01 2.62884218e-02]\n [2.55190535e-03 9.66625571e-01 1.53752625e-01 1.00000000e+00]\n [1.41548156e-03 1.41049293e-03 7.64848232e-01 2.69351900e-01]\n [5.04901111e-01 3.60784888e-01 6.37663364e-01 3.85480136e-01]\n [4.83383805e-01 6.19484127e-01 5.62658012e-01 6.61572099e-01]\n [4.98201400e-01 3.64614069e-01 6.61157548e-01 4.04896379e-01]\n [6.31229341e-01 3.60322893e-01 7.04147041e-01 4.11499381e-01]\n [5.21806777e-01 5.77694893e-01 5.87613106e-01 6.00717843e-01]\n [2.19603702e-01 3.48738879e-01 3.38255554e-01 3.77067655e-01]\n [1.24826752e-01 2.50923932e-01 2.79914767e-01 2.81625867e-01]\n [2.57318467e-01 5.67493618e-01 5.30910015e-01 6.87876582e-01]\n [4.21753637e-02 8.74765277e-01 2.52863318e-01 9.13046181e-01]\n [1.56401634e-01 4.43365514e-01 2.22233817e-01 4.75784540e-01]\n [5.01994431e-01 9.21467423e-01 8.36361706e-01 1.00000000e+00]\n [5.23673594e-01 5.70347011e-01 5.84506094e-01 5.91607034e-01]\n [5.19169092e-01 5.99966049e-01 6.46330178e-01 6.34094656e-01]\n [5.13154805e-01 6.79228544e-01 5.50981283e-01 6.92548096e-01]\n [5.24344563e-01 9.24945474e-01 8.10528100e-01 9.97979462e-01]\n [6.38063252e-01 4.42797273e-01 7.01729059e-01 4.84131962e-01]\n [3.41055505e-02 3.55657607e-01 1.62304893e-01 3.74908745e-01]\n [4.88090277e-01 4.53366935e-01 6.22257173e-01 4.79664922e-01]\n [9.66494903e-04 3.07707310e-01 1.06515847e-01 3.32070351e-01]\n [4.82970089e-01 6.19791627e-01 5.64778984e-01 6.60652578e-01]\n [5.82391143e-01 3.64923418e-01 7.13891625e-01 4.84685361e-01]\n [5.23789942e-01 7.49292731e-01 5.85470319e-01 7.65311480e-01]\n [3.51464331e-01 9.74868774e-01 5.53043604e-01 9.98887062e-01]\n [6.09076858e-01 4.26833510e-01 7.05196321e-01 4.87107515e-01]\n [5.69254696e-01 3.59782994e-01 7.08566308e-01 4.28438693e-01]\n [0.00000000e+00 8.11187327e-01 6.93582833e-01 9.93253708e-01]\n [1.04294335e-02 2.29470059e-02 7.27312565e-01 4.22287524e-01]\n [4.84632224e-01 4.10697758e-01 6.94742799e-01 4.63139951e-01]\n [8.11544433e-02 3.84775937e-01 2.07952142e-01 4.11755383e-01]\n [5.38567305e-01 6.03585005e-01 6.34740829e-01 6.34476542e-01]\n [0.00000000e+00 1.24075869e-02 1.40296444e-01 2.47341208e-02]\n [6.29779935e-01 6.14883304e-01 6.44907892e-01 6.25334918e-01]\n [5.02842903e-01 3.82420719e-01 5.96016407e-01 4.12718743e-01]\n [5.14681458e-01 7.47871041e-01 5.91947794e-01 7.66782522e-01]\n [5.06433249e-01 5.00402749e-01 6.00716949e-01 5.23319662e-01]\n [0.00000000e+00 2.11128622e-01 6.50825799e-01 4.34384257e-01]\n [0.00000000e+00 7.06320584e-01 6.17161393e-01 8.65940213e-01]\n [4.89298195e-01 4.54274893e-01 5.72620273e-01 4.76397544e-01]\n [5.09207368e-01 4.16264892e-01 6.69016659e-01 4.59577113e-01]\n [4.67802072e-03 8.03107023e-01 1.59582287e-01 8.40365171e-01]\n [5.26175678e-01 5.68375826e-01 5.79436243e-01 5.82803071e-01]\n [5.02847552e-01 3.73985887e-01 6.47125900e-01 4.12972569e-01]\n [4.85917568e-01 4.44437265e-01 6.24690175e-01 4.73519832e-01]\n [5.74168622e-01 2.67251313e-01 6.57761574e-01 3.20314020e-01]\n [6.71982348e-01 9.40317750e-01 8.21177185e-01 9.89214003e-01]\n [5.24104714e-01 5.61555982e-01 5.78347087e-01 5.80502510e-01]\n [5.17589748e-01 7.57220685e-01 5.88313937e-01 7.71545827e-01]\n [5.23328543e-01 5.57813823e-01 5.79028904e-01 5.73553503e-01]\n [6.12360001e-01 4.27401572e-01 7.06096232e-01 4.88300264e-01]\n [0.00000000e+00 2.44237110e-01 6.08887747e-02 2.93773860e-01]\n [1.54844411e-02 1.94191094e-03 7.45163262e-01 2.59336472e-01]\n [4.93266463e-01 9.23959553e-01 8.36913109e-01 9.97706771e-01]\n [5.05292952e-01 3.60166401e-01 6.43362343e-01 3.91438454e-01]\n [8.43423419e-03 2.42121428e-01 4.97449562e-02 2.83145577e-01]\n [5.22109151e-01 5.36088109e-01 5.97674847e-01 5.53133190e-01]\n [5.13126016e-01 5.23810089e-01 6.00540400e-01 5.42965055e-01]\n [5.18315673e-01 5.03453434e-01 5.97545326e-01 5.22752881e-01]\n [5.20455718e-01 6.00931644e-01 6.45991087e-01 6.34363830e-01]\n [5.13168275e-01 6.79253817e-01 5.50486088e-01 6.92442834e-01]\n [4.29723203e-01 8.28743577e-01 5.90048730e-01 8.64375412e-01]\n [5.26593328e-01 6.27190828e-01 5.63289881e-01 6.53784990e-01]\n [5.04781067e-01 3.89410675e-01 6.15231156e-01 4.19951618e-01]\n [5.01324892e-01 3.64236206e-01 6.59752846e-01 4.03719962e-01]\n [5.73110223e-01 2.66732633e-01 6.66223526e-01 3.18649918e-01]\n [5.15103400e-01 6.24091804e-01 5.63832283e-01 6.58031821e-01]\n [8.32032263e-02 4.07567918e-01 5.84344149e-01 5.58310449e-01]\n [2.88201928e-01 4.62544849e-04 4.14279878e-01 3.67076807e-02]\n [6.27132714e-01 3.60995084e-01 7.05960631e-01 4.09780413e-01]\n [4.97159481e-01 4.55211073e-01 5.84271312e-01 4.77872074e-01]\n [1.17194215e-02 3.08072537e-01 9.73200351e-02 3.25075477e-01]\n [5.15893996e-01 3.80090386e-01 5.96972466e-01 4.11767155e-01]\n [5.12429118e-01 6.23649299e-01 5.62436640e-01 6.57682240e-01]\n [4.00773793e-01 8.84974241e-01 5.81656575e-01 9.39130306e-01]\n [0.00000000e+00 9.94758867e-03 1.36253998e-01 3.15974355e-02]\n [5.13905644e-01 5.29502392e-01 6.02055967e-01 5.52376091e-01]\n [5.10691643e-01 6.24039650e-01 5.63410044e-01 6.58179879e-01]\n [4.80379909e-01 6.20327771e-01 5.65284193e-01 6.60123348e-01]\n [5.38407385e-01 9.28024352e-01 7.13617563e-01 9.99452770e-01]\n [4.86337811e-01 6.20247304e-01 5.63528776e-01 6.60217702e-01]]\n","output_type":"stream"}]}]}